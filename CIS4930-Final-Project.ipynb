{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec14af09-62a3-46f3-9f89-1ce9bdf51f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\nickm\\AppData\\Local\\Temp\\ipykernel_2756\\978055655.py:10: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  df = pd.read_csv(path + \"\\MIT-BIH Arrhythmia Database.csv\")\n",
      "c:\\Nicholas\\UF\\MachineLearning\\CIS4930-Final-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\nickm\\.cache\\kagglehub\\datasets\\sadmansakib7\\ecg-arrhythmia-classification-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sadmansakib7/ecg-arrhythmia-classification-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(path + \"\\MIT-BIH Arrhythmia Database.csv\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7e40c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (100689, 34)\n",
      "\n",
      "   record type  0_pre-RR  0_post-RR   0_pPeak   0_tPeak   0_rPeak   0_sPeak  \\\n",
      "0     101    N        76      313.0  0.074347 -0.160548  1.036401 -0.285662   \n",
      "1     101    N       313      315.0 -0.052079 -0.264784  0.886597 -0.366298   \n",
      "2     101    N       315      321.0 -0.062151 -0.296983  0.991859 -0.410306   \n",
      "3     101    N       321      336.0 -0.063322 -0.281386  1.034903 -0.403880   \n",
      "4     101    N       336      344.0 -0.062915  1.046914  1.046408  1.046408   \n",
      "\n",
      "    0_qPeak  0_qrs_interval  ...   1_qPeak  1_qrs_interval  1_pq_interval  \\\n",
      "0 -0.026824              41  ...  0.025930               2             18   \n",
      "1 -0.059710              21  ... -0.042009              26             27   \n",
      "2 -0.065686              22  ...  0.009528               3              8   \n",
      "3 -0.071750              22  ... -0.020536               6              9   \n",
      "4 -0.074639              11  ...  0.016053              16              5   \n",
      "\n",
      "   1_qt_interval  1_st_interval  1_qrs_morph0  1_qrs_morph1  1_qrs_morph2  \\\n",
      "0             22              2      0.025930      0.025930      0.025930   \n",
      "1             62              9     -0.042009     -0.029498      0.005012   \n",
      "2             12              1      0.009528      0.009528      0.008786   \n",
      "3             16              1     -0.020536     -0.020257     -0.018965   \n",
      "4             31             10      0.016053      0.006742      0.002782   \n",
      "\n",
      "   1_qrs_morph3  1_qrs_morph4  \n",
      "0      0.025436      0.025436  \n",
      "1      0.030892      0.002986  \n",
      "2      0.008786      0.008368  \n",
      "3     -0.016968     -0.014555  \n",
      "4     -0.007798     -0.051155  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100689 entries, 0 to 100688\n",
      "Data columns (total 34 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   record          100689 non-null  int64  \n",
      " 1   type            100689 non-null  object \n",
      " 2   0_pre-RR        100689 non-null  int64  \n",
      " 3   0_post-RR       100689 non-null  float64\n",
      " 4   0_pPeak         100689 non-null  float64\n",
      " 5   0_tPeak         100689 non-null  float64\n",
      " 6   0_rPeak         100689 non-null  float64\n",
      " 7   0_sPeak         100689 non-null  float64\n",
      " 8   0_qPeak         100689 non-null  float64\n",
      " 9   0_qrs_interval  100689 non-null  int64  \n",
      " 10  0_pq_interval   100689 non-null  int64  \n",
      " 11  0_qt_interval   100689 non-null  int64  \n",
      " 12  0_st_interval   100689 non-null  int64  \n",
      " 13  0_qrs_morph0    100689 non-null  float64\n",
      " 14  0_qrs_morph1    100689 non-null  float64\n",
      " 15  0_qrs_morph2    100689 non-null  float64\n",
      " 16  0_qrs_morph3    100689 non-null  float64\n",
      " 17  0_qrs_morph4    100689 non-null  float64\n",
      " 18  1_pre-RR        100689 non-null  int64  \n",
      " 19  1_post-RR       100689 non-null  float64\n",
      " 20  1_pPeak         100689 non-null  float64\n",
      " 21  1_tPeak         100689 non-null  float64\n",
      " 22  1_rPeak         100689 non-null  float64\n",
      " 23  1_sPeak         100689 non-null  float64\n",
      " 24  1_qPeak         100689 non-null  float64\n",
      " 25  1_qrs_interval  100689 non-null  int64  \n",
      " 26  1_pq_interval   100689 non-null  int64  \n",
      " 27  1_qt_interval   100689 non-null  int64  \n",
      " 28  1_st_interval   100689 non-null  int64  \n",
      " 29  1_qrs_morph0    100689 non-null  float64\n",
      " 30  1_qrs_morph1    100689 non-null  float64\n",
      " 31  1_qrs_morph2    100689 non-null  float64\n",
      " 32  1_qrs_morph3    100689 non-null  float64\n",
      " 33  1_qrs_morph4    100689 non-null  float64\n",
      "dtypes: float64(22), int64(11), object(1)\n",
      "memory usage: 26.1+ MB\n",
      "Index(['record', 'type', '0_pre-RR', '0_post-RR', '0_pPeak', '0_tPeak',\n",
      "       '0_rPeak', '0_sPeak', '0_qPeak', '0_qrs_interval', '0_pq_interval',\n",
      "       '0_qt_interval', '0_st_interval', '0_qrs_morph0', '0_qrs_morph1',\n",
      "       '0_qrs_morph2', '0_qrs_morph3', '0_qrs_morph4', '1_pre-RR', '1_post-RR',\n",
      "       '1_pPeak', '1_tPeak', '1_rPeak', '1_sPeak', '1_qPeak', '1_qrs_interval',\n",
      "       '1_pq_interval', '1_qt_interval', '1_st_interval', '1_qrs_morph0',\n",
      "       '1_qrs_morph1', '1_qrs_morph2', '1_qrs_morph3', '1_qrs_morph4'],\n",
      "      dtype='object')\n",
      "type\n",
      "N       90083\n",
      "VEB      7009\n",
      "SVEB     2779\n",
      "F         803\n",
      "Q          15\n",
      "Name: count, dtype: int64\n",
      "{'F': np.int64(0), 'N': np.int64(1), 'SVEB': np.int64(2), 'VEB': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# This code block is for the data exploration and visualization\n",
    "\n",
    "# N (Normal):\n",
    "# Description: Represents normal heartbeats. These are the most common and indicate a regular, healthy heartbeat pattern.\n",
    "# Count: 90,083 instances in your dataset, indicating that normal heartbeats are the majority class.\n",
    "# VEB (Ventricular Ectopic Beat):\n",
    "# Description: These are premature heartbeats originating from the ventricles.\n",
    "# Count: 7,009 instances, making it a minority class compared to normal beats.\n",
    "# SVEB (Supraventricular Ectopic Beat):\n",
    "# Description: These are premature heartbeats originating above the ventricles, often in the atria.\n",
    "# Count: 2,779 instances, another minority class.\n",
    "# F (Fusion Beat):\n",
    "# Description: Fusion beats occur when a normal heartbeat and an ectopic beat occur at the same time\n",
    "# Count: 803 instances, indicating it's a relatively rare occurrence in your dataset.\n",
    "# Q (Unknown/Unclassified):\n",
    "# Description: This category might represent beats that couldn't be classified into the other categories\n",
    "# Count: 15 instances, making it the rarest class in your dataset, should be removed\n",
    "\n",
    "# data visualization and preprocessing\n",
    "# bar chart of class distribution\n",
    "df['type'].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Data exploration\n",
    "print(f'Dataset Shape: {df.shape}\\n')\n",
    "print(df.head(5))\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "#columns in dataset:\n",
    "print(df.columns)\n",
    "print(df['type'].value_counts()) \n",
    "\n",
    "# from the visualization, we can see that there are some classes with very few samples, and classes that don't do anything\n",
    "df = df.drop(columns=['record'])\n",
    "\n",
    "# drop type ==  Q, not enough samples to form worthwhile predictions\n",
    "df = df[df['type'] != 'Q']\n",
    "\n",
    "# Encode the 'type' column,  because the classes are not ordinal\n",
    "label_encoder = LabelEncoder()\n",
    "df['type'] = label_encoder.fit_transform(df['type'])\n",
    "\n",
    "# Print the mapping of classes to encoded values\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "# X contains all columns except 'type', which is our target variable\n",
    "# y contains only the 'type' column which has been encoded to numeric values\n",
    "X = df.drop('type', axis=1)\n",
    "y = df['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training data, to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71160486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8207 - loss: 0.5267 - val_accuracy: 0.8419 - val_loss: 0.4098\n",
      "Epoch 2/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1928 - val_accuracy: 0.8969 - val_loss: 0.2929\n",
      "Epoch 3/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9669 - loss: 0.1312 - val_accuracy: 0.9235 - val_loss: 0.2463\n",
      "Epoch 4/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9752 - loss: 0.0984 - val_accuracy: 0.9480 - val_loss: 0.1688\n",
      "Epoch 5/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0771 - val_accuracy: 0.9427 - val_loss: 0.2050\n",
      "Epoch 6/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9828 - loss: 0.0678 - val_accuracy: 0.9587 - val_loss: 0.1410\n",
      "Epoch 7/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0552 - val_accuracy: 0.9519 - val_loss: 0.1644\n",
      "Epoch 8/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9870 - loss: 0.0506 - val_accuracy: 0.9802 - val_loss: 0.0734\n",
      "Epoch 9/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0449 - val_accuracy: 0.9708 - val_loss: 0.0997\n",
      "Epoch 10/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0407 - val_accuracy: 0.9795 - val_loss: 0.0806\n",
      "\u001b[1m2832/2832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "Epoch 1/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.3955 - val_accuracy: 0.9051 - val_loss: 0.4341\n",
      "Epoch 2/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0668 - val_accuracy: 0.9456 - val_loss: 0.2086\n",
      "Epoch 3/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0448 - val_accuracy: 0.9635 - val_loss: 0.1190\n",
      "Epoch 4/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0290 - val_accuracy: 0.9670 - val_loss: 0.1120\n",
      "Epoch 5/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0259 - val_accuracy: 0.9829 - val_loss: 0.0549\n",
      "Epoch 6/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0251 - val_accuracy: 0.9737 - val_loss: 0.0925\n",
      "Epoch 7/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.0175 - val_accuracy: 0.9843 - val_loss: 0.0516\n",
      "Epoch 8/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0209 - val_accuracy: 0.9840 - val_loss: 0.0531\n",
      "Epoch 9/20\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0190 - val_accuracy: 0.9818 - val_loss: 0.0535\n",
      "\u001b[1m2832/2832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "Model Results sorted by weighted f1-score:\n",
      "\n",
      "XGBoost Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.88      0.80      0.84       720\n",
      "           N       0.99      0.99      0.99     81101\n",
      "        SVEB       0.89      0.86      0.87      2476\n",
      "         VEB       0.95      0.96      0.96      6310\n",
      "\n",
      "    accuracy                           0.99     90607\n",
      "   macro avg       0.93      0.90      0.91     90607\n",
      "weighted avg       0.99      0.99      0.99     90607\n",
      "\n",
      "Weighted F1-Score: 0.9863\n",
      "\n",
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.90      0.74      0.81       720\n",
      "           N       0.99      0.99      0.99     81101\n",
      "        SVEB       0.87      0.84      0.86      2476\n",
      "         VEB       0.92      0.96      0.94      6310\n",
      "\n",
      "    accuracy                           0.98     90607\n",
      "   macro avg       0.92      0.88      0.90     90607\n",
      "weighted avg       0.98      0.98      0.98     90607\n",
      "\n",
      "Weighted F1-Score: 0.9827\n",
      "\n",
      "KNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.59      0.78      0.67       720\n",
      "           N       0.99      0.98      0.98     81101\n",
      "        SVEB       0.65      0.88      0.75      2476\n",
      "         VEB       0.91      0.94      0.92      6310\n",
      "\n",
      "    accuracy                           0.97     90607\n",
      "   macro avg       0.79      0.89      0.83     90607\n",
      "weighted avg       0.97      0.97      0.97     90607\n",
      "\n",
      "Weighted F1-Score: 0.9715\n",
      "\n",
      "DNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.42      0.81      0.56       720\n",
      "           N       0.99      0.98      0.98     81101\n",
      "        SVEB       0.75      0.87      0.81      2476\n",
      "         VEB       0.89      0.94      0.92      6310\n",
      "\n",
      "    accuracy                           0.97     90607\n",
      "   macro avg       0.77      0.90      0.82     90607\n",
      "weighted avg       0.97      0.97      0.97     90607\n",
      "\n",
      "Weighted F1-Score: 0.9710\n",
      "\n",
      "SNN Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.58      0.77      0.66       720\n",
      "           N       0.99      0.97      0.98     81101\n",
      "        SVEB       0.59      0.87      0.70      2476\n",
      "         VEB       0.89      0.93      0.91      6310\n",
      "\n",
      "    accuracy                           0.96     90607\n",
      "   macro avg       0.76      0.89      0.81     90607\n",
      "weighted avg       0.97      0.96      0.97     90607\n",
      "\n",
      "Weighted F1-Score: 0.9666\n",
      "\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.50      0.80      0.62       720\n",
      "           N       0.99      0.96      0.98     81101\n",
      "        SVEB       0.49      0.91      0.64      2476\n",
      "         VEB       0.90      0.95      0.93      6310\n",
      "\n",
      "    accuracy                           0.96     90607\n",
      "   macro avg       0.72      0.90      0.79     90607\n",
      "weighted avg       0.97      0.96      0.96     90607\n",
      "\n",
      "Weighted F1-Score: 0.9610\n",
      "\n",
      "SVM Linear Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.09      0.86      0.16       720\n",
      "           N       0.99      0.81      0.89     81101\n",
      "        SVEB       0.21      0.85      0.33      2476\n",
      "         VEB       0.76      0.87      0.81      6310\n",
      "\n",
      "    accuracy                           0.82     90607\n",
      "   macro avg       0.51      0.85      0.55     90607\n",
      "weighted avg       0.95      0.82      0.87     90607\n",
      "\n",
      "Weighted F1-Score: 0.8653\n",
      "\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.10      0.85      0.18       720\n",
      "           N       0.99      0.79      0.88     81101\n",
      "        SVEB       0.18      0.84      0.29      2476\n",
      "         VEB       0.68      0.85      0.76      6310\n",
      "\n",
      "    accuracy                           0.80     90607\n",
      "   macro avg       0.49      0.83      0.53     90607\n",
      "weighted avg       0.94      0.80      0.85     90607\n",
      "\n",
      "Weighted F1-Score: 0.8517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# This code block is foor all the individual baseline models\n",
    "model_results = {}\n",
    "\n",
    "# Function to get weighted avg f1-score from classification report\n",
    "def get_weighted_f1(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "    return report['weighted avg']['f1-score']\n",
    "\n",
    "# Initialize XGBoost classifier with specific parameters\n",
    "# use_label_encoder=False: Avoid using the deprecated label encoder\n",
    "# eval_metric='mlogloss': Use multiclass log loss as evaluation metric\n",
    "# verbosity=0: Suppress verbose output\n",
    "# random_state=42: Set seed for reproducibility\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', verbosity=0, random_state=42)\n",
    "\n",
    "# Train the XGBoost model on the training data\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Use the original class names from the label encoder for better readability in the report\n",
    "model_results['XGBoost'] = {'report': classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_),\n",
    "                           'weighted_f1': get_weighted_f1(y_test, y_pred_xgb)}\n",
    "\n",
    "# Initialize Random Forest classifier with specific parameters\n",
    "# n_estimators=25: Use 25 trees in the forest\n",
    "# random_state=42: Set seed for reproducibility\n",
    "rf_model = RandomForestClassifier(n_estimators=25, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the Random Forest model on the training data\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the classification report\n",
    "model_results['Random Forest'] = {'report': classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_),\n",
    "                                  'weighted_f1': get_weighted_f1(y_test, y_pred_rf)}\n",
    "\n",
    "# Initialize the SVM classifier with RBF kernel for non-linear data\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the SVM model on the resampled training data\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "model_results['SVM'] = {'report': classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_),\n",
    "                        'weighted_f1': get_weighted_f1(y_test, y_pred_svm)}\n",
    "\n",
    "# Now, a SVM classifier with linear kernel\n",
    "svm_linear_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the SVM model on the resampled training data\n",
    "svm_linear_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm_linear = svm_linear_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "model_results['SVM Linear'] = {'report': classification_report(y_test, y_pred_svm_linear, target_names=label_encoder.classes_),\n",
    "                               'weighted_f1': get_weighted_f1(y_test, y_pred_svm_linear)}\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train the Logistic Regression model on the resampled training data\n",
    "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "model_results['Logistic Regression'] = {'report': classification_report(y_test, y_pred_logistic, target_names=label_encoder.classes_),\n",
    "                                        'weighted_f1': get_weighted_f1(y_test, y_pred_logistic)}\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the KNN model on the resampled training data\n",
    "knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "model_results['KNN'] = {'report': classification_report(y_test, y_pred_knn, target_names=label_encoder.classes_),\n",
    "                        'weighted_f1': get_weighted_f1(y_test, y_pred_knn)}\n",
    "\n",
    "# Neural Network (Shallow)\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Input # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "num_classes = len(np.unique(y_train_resampled))\n",
    "n_features = X_train_resampled.shape[1]\n",
    "\n",
    "# Instantiate\n",
    "shallow_model = Sequential()\n",
    "\n",
    "# Declare Input Shape\n",
    "shallow_model.add(Input(shape=(n_features,)))\n",
    "\n",
    "# Hidden Layer\n",
    "shallow_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "shallow_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "shallow_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "shallow_model.fit(X_train_resampled, y_train_resampled, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_shallow = shallow_model.predict(X_test_scaled)\n",
    "y_pred_shallow = np.argmax(y_pred_shallow, axis=1)\n",
    "\n",
    "# Evaluate\n",
    "model_results['SNN'] = {'report': classification_report(y_test, y_pred_shallow, target_names=label_encoder.classes_),\n",
    "                        'weighted_f1': get_weighted_f1(y_test, y_pred_shallow)}\n",
    "\n",
    "# Neural Network (Deep)\n",
    "num_classes = len(np.unique(y_train_resampled))\n",
    "n_features = X_train_resampled.shape[1]\n",
    "\n",
    "# Instantiate\n",
    "deep_model = Sequential()\n",
    "\n",
    "# Declare Input Shape\n",
    "deep_model.add(Input(shape=(n_features,)))\n",
    "\n",
    "# Hidden Layers (4)\n",
    "deep_model.add(Dense(128, activation='relu'))\n",
    "deep_model.add(Dense(64, activation='relu'))\n",
    "deep_model.add(Dense(32, activation='relu'))\n",
    "deep_model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "deep_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "deep_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "deep_model.fit(X_train_resampled, y_train_resampled, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_deep = deep_model.predict(X_test_scaled)\n",
    "y_pred_deep = np.argmax(y_pred_deep, axis=1)\n",
    "\n",
    "# Evaluate\n",
    "model_results['DNN'] = {'report': classification_report(y_test, y_pred_deep, target_names=label_encoder.classes_),\n",
    "                        'weighted_f1': get_weighted_f1(y_test, y_pred_deep)}\n",
    "\n",
    "# Sort the model results by weighted f1-score\n",
    "sorted_model_results = sorted(model_results.items(), key=lambda x: x[1]['weighted_f1'], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Results sorted by weighted f1-score:\")\n",
    "for model, result in sorted_model_results:\n",
    "    print(f\"\\n{model} Results:\")\n",
    "    print(result['report'])\n",
    "    print(f\"Weighted F1-Score: {result['weighted_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9b44c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Results sorted by weighted f1-score:\n",
      "\n",
      "Stacking Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.90      0.76      0.82       720\n",
      "           N       0.99      1.00      0.99     81101\n",
      "        SVEB       0.94      0.83      0.88      2476\n",
      "         VEB       0.97      0.95      0.96      6310\n",
      "\n",
      "    accuracy                           0.99     90607\n",
      "   macro avg       0.95      0.88      0.91     90607\n",
      "weighted avg       0.99      0.99      0.99     90607\n",
      "\n",
      "Weighted F1-Score: 0.9864\n",
      "\n",
      "Soft Voting Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.85      0.78      0.81       720\n",
      "           N       0.99      0.99      0.99     81101\n",
      "        SVEB       0.88      0.88      0.88      2476\n",
      "         VEB       0.95      0.96      0.95      6310\n",
      "\n",
      "    accuracy                           0.99     90607\n",
      "   macro avg       0.92      0.90      0.91     90607\n",
      "weighted avg       0.99      0.99      0.99     90607\n",
      "\n",
      "Weighted F1-Score: 0.9858\n",
      "\n",
      "Voting Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.85      0.79      0.82       720\n",
      "           N       0.99      0.99      0.99     81101\n",
      "        SVEB       0.89      0.87      0.88      2476\n",
      "         VEB       0.95      0.96      0.95      6310\n",
      "\n",
      "    accuracy                           0.99     90607\n",
      "   macro avg       0.92      0.90      0.91     90607\n",
      "weighted avg       0.99      0.99      0.99     90607\n",
      "\n",
      "Weighted F1-Score: 0.9857\n",
      "\n",
      "GradientBoosting Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.89      0.79      0.84       720\n",
      "           N       0.99      0.99      0.99     81101\n",
      "        SVEB       0.88      0.85      0.87      2476\n",
      "         VEB       0.94      0.96      0.95      6310\n",
      "\n",
      "    accuracy                           0.99     90607\n",
      "   macro avg       0.93      0.90      0.91     90607\n",
      "weighted avg       0.98      0.99      0.98     90607\n",
      "\n",
      "Weighted F1-Score: 0.9850\n",
      "\n",
      "Bagging Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.49      0.70      0.58       720\n",
      "           N       0.99      0.98      0.98     81101\n",
      "        SVEB       0.72      0.80      0.76      2476\n",
      "         VEB       0.89      0.91      0.90      6310\n",
      "\n",
      "    accuracy                           0.97     90607\n",
      "   macro avg       0.77      0.85      0.80     90607\n",
      "weighted avg       0.97      0.97      0.97     90607\n",
      "\n",
      "Weighted F1-Score: 0.9685\n",
      "\n",
      "AdaBoost Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.44      0.66      0.53       720\n",
      "           N       0.99      0.97      0.98     81101\n",
      "        SVEB       0.61      0.78      0.68      2476\n",
      "         VEB       0.84      0.88      0.86      6310\n",
      "\n",
      "    accuracy                           0.96     90607\n",
      "   macro avg       0.72      0.82      0.76     90607\n",
      "weighted avg       0.96      0.96      0.96     90607\n",
      "\n",
      "Weighted F1-Score: 0.9580\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "ensemble_results = {}\n",
    "\n",
    "# we are using the best 3 models to create an ensemble model, which is XGBoost, Random Forest, and KNN\n",
    "# This code block is for all the ensemble models\n",
    "# Create a VotingClassifier\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('knn', knn_model)\n",
    "    ],\n",
    "    voting='hard'  # 'hard' for majority voting, 'soft' for averaging probabilities\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "voting_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_voting = voting_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_results['Voting Classifier'] = {'report': classification_report(y_test, y_pred_voting, target_names=label_encoder.classes_),\n",
    "                                        'weighted_f1': get_weighted_f1(y_test, y_pred_voting)}\n",
    "\n",
    "# Soft Voting results\n",
    "soft_voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('knn', knn_model)\n",
    "    ],  \n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for averaging probabilities\n",
    ")\n",
    "\n",
    "# Train the soft voting model\n",
    "soft_voting_model.fit(X_train_resampled, y_train_resampled) \n",
    "\n",
    "# Make predictions\n",
    "y_pred_soft_voting = soft_voting_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the soft voting model\n",
    "ensemble_results['Soft Voting'] = {'report': classification_report(y_test, y_pred_soft_voting, target_names=label_encoder.classes_),\n",
    "                                  'weighted_f1': get_weighted_f1(y_test, y_pred_soft_voting)}\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),   \n",
    "        ('knn', knn_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=42, max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)   \n",
    "\n",
    "# Make predictions\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "ensemble_results['Stacking Classifier'] = {'report': classification_report(y_test, y_pred_stacking, target_names=label_encoder.classes_),\n",
    "                                         'weighted_f1': get_weighted_f1(y_test, y_pred_stacking)}\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")   \n",
    "\n",
    "# Train the bagging model\n",
    "bagging_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bagging = bagging_model.predict(X_test_scaled)   \n",
    "\n",
    "# Evaluate the bagging model\n",
    "ensemble_results['Bagging Classifier'] = {'report': classification_report(y_test, y_pred_bagging, target_names=label_encoder.classes_),\n",
    "                                         'weighted_f1': get_weighted_f1(y_test, y_pred_bagging)}        \n",
    "\n",
    "# AdaBoost Classifier\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")           \n",
    "\n",
    "# Train the AdaBoost model\n",
    "ada_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = ada_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the AdaBoost model\n",
    "ensemble_results['AdaBoost Classifier'] = {'report': classification_report(y_test, y_pred_ada, target_names=label_encoder.classes_),\n",
    "                                          'weighted_f1': get_weighted_f1(y_test, y_pred_ada)}\n",
    "\n",
    "# Fine-tune parameters\n",
    "gradient_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,  # Add randomness\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the GradientBoosting model\n",
    "gradient_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gradient = gradient_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the GradientBoosting model\n",
    "ensemble_results['GradientBoosting Classifier'] = {'report': classification_report(y_test, y_pred_gradient, target_names=label_encoder.classes_),\n",
    "                                                  'weighted_f1': get_weighted_f1(y_test, y_pred_gradient)}\n",
    "\n",
    "# Print the results\n",
    "sorted_ensemble_results = sorted(ensemble_results.items(), key=lambda x: x[1]['weighted_f1'], reverse=True)\n",
    "print(\"Ensemble Model Results sorted by weighted f1-score:\")\n",
    "for model, result in sorted_ensemble_results:\n",
    "    print(f\"\\n{model} Results:\")\n",
    "    print(result['report'])\n",
    "    print(f\"Weighted F1-Score: {result['weighted_f1']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
